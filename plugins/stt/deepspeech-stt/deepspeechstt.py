import deepspeech
import numpy
import os
import scipy.io.wavfile as wav
import urllib
from collections import OrderedDict
from naomi import paths
from naomi import plugin
from naomi import pluginstore
from naomi import profile
from naomi.run_command import run_command
from pprint import pprint
import pdb
import slugify


def generate_scorer(language_model_path):
    version="v{}".format(deepspeech.version())
    sourcedir_name="STT{}".format(version)
    sourcedir_path=paths.sub(os.path.join("sources", sourcedir_name))    
    kenlm_sourcedir = paths.sub(os.path.join("sources", "kenlm"))
    kenlm_bins = os.path.join(kenlm_sourcedir, 'build', 'bin')
    cmd = [
        'python',
        os.path.join(sourcedir_path, 'data', 'lm', 'generate_lm.py'),
        '--input_txt', os.path.join(language_model_path, 'phrases.txt'),
        '--output_dir', language_model_path,
        '--kenlm_bins', kenlm_bins,
        '--binary_type', 'trie',
        '--top_k', '500000',
        '--arpa_order', '5',
        '--max_arpa_memory', '85%',
        '--arpa_prune', '0|0|1',
        '--binary_a_bits', '255',
        '--binary_q_bits', '8',
        '--discount_fallback'
    ]
    completed_process = run_command(cmd, 2)
    if(completed_process.returncode == 0):
        # There should be an additional step here where the
        # default values of alpha and beta are generated by
        # lm_optimizer. However, that involves building a
        # dev set from data in the audiolog.
        # Generate the scorer package
        # I'm still trying to figure out where to get the
        # generate_scorer_package binary. I found mine in the
        # ~/.cache/bazel folder after running bazel build
        # on //native_client:generate_scorer_package
        # I think there are binary packages available on
        # github, but you have to know your processor type,
        # and since I am testing on Raspbianx86, I have to
        # build it. x86_64 or rpi are available on Github.
        print("Generating scorer package")
        cmd = [
            'generate_scorer_package',
            '--alphabet', os.path.join(sourcedir_path, 'data', 'alphabet.txt'),
            '--lm', os.path.join(language_model_path, 'lm.binary'),
            '--vocab', os.path.join(language_model_path, 'vocab-500000.txt'),
            '--package', os.path.join(language_model_path, 'scorer'),
            '--default_alpha', '0.931289105002',
            '--default_beta', '1.18341375810284'
        ]
        completed_process = run_command(cmd, 2)
        if(completed_process.returncode != 0):
            print(completed_process.stderr.decode("UTF-8"))
    else:
        print(completed_process.stderr.decode("UTF-8"))


class DeepSpeechSTTPlugin(plugin.STTPlugin):
    """
    Speech-To-Text implementation which relies on the DeepSpeech API.
    We want to generate custom language models for different conversation
    levels. These levels include a general level to identify the intent,
    an intent level, and a "special" level in which either a specific
    response or one of a limited number of alternatives is expected.
    """
    language_models_dir = paths.sub(os.path.join('languagemodels'))

    # is_keyword just checks to see if the word is a normal word or a keyword
    # (surrounded by curly brackets)
    @staticmethod
    def is_keyword(word):
        word = word.strip()
        response = False
        if("{}{}".format(word[:1], word[-1:]) == "{}"):
            response = True
        return response
    
    @staticmethod
    def keyword(word):
        return "{}{}{}".format("{", word, "}")

    def __init__(self, *args, **kwargs):
        """
        Create Plugin Instance
        """
        plugin.STTPlugin.__init__(self, *args, **kwargs)
        # Make sure the language models directory exists
        if not os.path.isdir(self.language_models_dir):
            os.makedirs(self.language_models_dir)

        # Check that we have the correct project source downloaded
        version="v{}".format(deepspeech.version())
        sourcedir_name="STT{}".format(version)
        sourcedir_path=paths.sub(os.path.join("sources", sourcedir_name))
        if(not os.path.isdir(sourcedir_path)):
            # use git to download the appropriate source directory
            cmd = [
                'git',
                'clone',
                '-b', version,
                'https://github.com/mozilla/STT',
                sourcedir_path
            ]
            completed_process = run_command(cmd, 2)
            if(completed_process.returncode != 0):
                print(completed_process.stderr.decode("UTF-8"))        
        kenlm_sourcedir = paths.sub(os.path.join("sources", "kenlm"))
        if(not os.path.isdir(kenlm_sourcedir)):
            # use git to download kenlm
            print("Cloning KenLM")
            cmd = [
                'git',
                'clone',
                'https://github.com/kpu/kenlm.git',
                kenlm_sourcedir
            ]
            completed_process = run_command(cmd, 2)
            if(completed_process.returncode == 0):
                # build kenlm
                print("Building KenLM")
                build_dir = os.path.join(kenlm_sourcedir,"build")
                if(not os.path.isdir(build_dir)):
                    os.makedirs(build_dir)
                    cmd = [
                        'cd', build_dir, '&&',
                        'cmake', '..', '&&',
                        'make'
                    ]
            else:
                print(completed_process.stderr.decode("UTF-8"))
        # Get a list of intents from the tti plugin and use kenlm to build scorers
        kenlm_bins = os.path.join(kenlm_sourcedir, 'build', 'bin')
        print("Generating base language model")
        generate_scorer(self.language_models_dir)
        for intent in profile.get_arg('tti_plugin').list_intents():
            print("Generating language model for intent: {}".format(intent))
            language_model_dir = os.path.join(self.language_models_dir, intent)
            generate_scorer(language_model_dir)

        # Beam width used in the CTC decoder when building candidate
        # transcriptions
        self._BEAM_WIDTH = profile.get(
            ['deepspeech', 'beam_width'],
            500
        )

        # Only 16KHz files are currently supported
        self._FS = profile.get(
            ['deepspeech', 'fs'],
            16000
        )

        # These are paths. They are required.
        # Path to the model (protocol buffer binary file)
        self._MODEL = os.path.join(
            os.path.expanduser(
                profile.get(
                    ['deepspeech', 'working_dir']
                )
            ),
            "model_{}.pbmm".format(version)
        )
        print("Model: {}".format(self._MODEL))
        if(not os.path.exists(self._MODEL)):
            download_url = 'https://github.com/mozilla/DeepSpeech/releases/download/{}/deepspeech-{}-models.pbmm'.format(version, deepspeech.version())
            filedata = urllib.request.urlopen(download_url)
            print("Downloading {}".format(download_url))
            # FIXME it would be good to have a progress indicator here.
            # This can take a long time depending on your bandwidth.
            with open(self._MODEL, 'wb') as f:
                f.write(filedata.read())
            print("Download completed")
        self._ds = deepspeech.Model(self._MODEL)

    def settings(self):
        _ = self.gettext
        return OrderedDict(
            [
                (
                    ('deepspeech', 'working_dir'), {
                        'title': _('DeepSpeech working directory'),
                        'description': "".join([
                            _('Contains the DeepSpeech model')
                        ]),
                        'default': paths.sub("deepspeech")
                    }
                )
            ]
        )

    def transcribe(self, fp, intent=None):
        """
        transcribe given audio file object fp and return the result.
        """
        fp.seek(0)
        fs, audio = wav.read(fp)
        # We can assume 16kHz
        # audio_length = len(audio) * (1 / self._FS)
        assert fs == self._FS, (
            "Input wav file is %dHz, expecting %dHz" % (fs, self._FS)
        )
        if(intent is None):
            self._ds.enableExternalScorer(os.path.join(self.language_models_dir, "scorer"))
        else:
            self._ds.enableExternalScorer(os.path.join(self.language_models_dir, intent, "scorer"))
        text = self._ds.stt(audio)

        transcribed = [text.upper()]

        return transcribed
